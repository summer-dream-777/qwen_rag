data:
  eval_batch_size: 4
  max_length: 512
  train_batch_size: 2
model:
  lora_config:
    lora_alpha: 32
    lora_dropout: 0.1
    r: 16
    target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    task_type: CAUSAL_LM
  name_or_path: experiments/20250811_025008/sft_debug/checkpoint-25
  use_lora: true
output_dir: experiments/{timestamp}/dpo_debug
training:
  beta: 0.1
  eval_steps: 10
  fp16: true
  gradient_accumulation_steps: 1
  label_smoothing: 0.0
  learning_rate: 1.0e-05
  logging_steps: 1
  num_epochs: 1
  save_steps: 10
  warmup_ratio: 0.1
